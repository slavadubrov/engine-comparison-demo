{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# PySpark on Docker Compose\n",
    "Distributed tabular ETL with Apache Spark — petabyte-scale joins with fault tolerance and Adaptive Query Execution (AQE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Start the Spark stack and launch Jupyter:\n",
    "\n",
    "```bash\n",
    "# 1. Build images\n",
    "docker compose build\n",
    "\n",
    "# 2. Start MinIO + Spark + App\n",
    "docker compose up -d minio minio-setup spark-master\n",
    "docker compose up -d --scale spark-worker=1 spark-worker app\n",
    "\n",
    "# 3. Upload sample data\n",
    "./scripts/upload-data.sh\n",
    "\n",
    "# 4. Launch Jupyter Lab\n",
    "docker compose exec app jupyter lab --ip 0.0.0.0 --port 8888 --allow-root --no-browser --notebook-dir=/app/notebook\n",
    "```\n",
    "\n",
    "Then open http://localhost:8888 in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "source": [
    "## What is Spark?\n",
    "\n",
    "Apache Spark is the de facto standard for **petabyte-scale tabular ETL**. Key concepts:\n",
    "\n",
    "- **SparkSession** — entry point to all Spark functionality\n",
    "- **DataFrames** — distributed collections with SQL-like API\n",
    "- **Lazy evaluation** — transformations build a DAG, actions trigger execution\n",
    "- **Adaptive Query Execution (AQE)** — runtime query re-optimization\n",
    "- **Fault tolerance** — automatic recovery via RDD lineage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "```\n",
    "Driver (app container) → Executor (spark-worker) → MinIO (S3 storage)\n",
    "```\n",
    "\n",
    "The driver plans and coordinates. Workers execute tasks in parallel. Data flows through MinIO as the shared storage layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Notebook_Spark_ETL\")\n",
    "    .master(\"spark://spark-master:7077\")\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://minio:9000\")\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", \"minioadmin\")\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\")\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "print(f\"Spark UI: http://localhost:8080\")\n",
    "print(f\"Connected to: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = spark.read.parquet(\"s3a://lake/taxi/*.parquet\")\n",
    "zones = (\n",
    "    spark.read.option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"s3a://lake/taxi/taxi_zone_lookup.csv\")\n",
    ")\n",
    "\n",
    "print(f\"Trips: {trips.count():,} rows\")\n",
    "trips.printSchema()\n",
    "trips.show(5)\n",
    "\n",
    "print(f\"\\nZones: {zones.count()} rows\")\n",
    "zones.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "source": [
    "## Filter Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# High-value trips: fare > $10 and distance > 5 miles\n",
    "filtered = trips.filter((F.col(\"fare_amount\") > 10.0) & (F.col(\"trip_distance\") > 5.0))\n",
    "print(f\"High-value trips: {filtered.count():,} (out of {trips.count():,})\")\n",
    "filtered.select(\"trip_distance\", \"fare_amount\", \"tip_amount\", \"total_amount\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "source": [
    "## GroupBy + Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue breakdown by payment type\n",
    "payment_stats = (\n",
    "    trips.groupBy(\"payment_type\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"trip_count\"),\n",
    "        F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"tip_amount\").alias(\"avg_tip\"),\n",
    "        F.avg(\"trip_distance\").alias(\"avg_distance\"),\n",
    "    )\n",
    "    .orderBy(F.desc(\"total_revenue\"))\n",
    ")\n",
    "payment_stats.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "source": [
    "## Join — Enrich with Zone Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join trips with zone lookup to get human-readable pickup locations\n",
    "enriched = trips.join(zones, trips[\"PULocationID\"] == zones[\"LocationID\"], \"inner\")\n",
    "enriched.select(\"Borough\", \"Zone\", \"fare_amount\", \"trip_distance\", \"tip_amount\").show(\n",
    "    10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bbdb311c014d738909a11f9e486628",
   "metadata": {},
   "source": [
    "## Window Functions + Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b363d81ae4b689946ece5c682cd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top zones per borough by revenue\n",
    "window = Window.partitionBy(\"Borough\").orderBy(F.desc(\"revenue\"))\n",
    "\n",
    "zone_revenue = (\n",
    "    enriched.groupBy(\"Borough\", \"Zone\")\n",
    "    .agg(\n",
    "        F.sum(\"total_amount\").alias(\"revenue\"),\n",
    "        F.count(\"*\").alias(\"trips\"),\n",
    "        F.avg(\"tip_amount\").alias(\"avg_tip\"),\n",
    "    )\n",
    "    .withColumn(\"rank\", F.row_number().over(window))\n",
    "    .filter(F.col(\"rank\") <= 3)\n",
    "    .orderBy(\"Borough\", \"rank\")\n",
    ")\n",
    "zone_revenue.show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65eabff63a45729fe45fb5ade58bdc",
   "metadata": {},
   "source": [
    "## Full ETL Pipeline\n",
    "\n",
    "Filter → Join → Aggregate → Rank → Write to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3933fab20d04ec698c2621248eb3be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "window = Window.partitionBy(\"Borough\").orderBy(F.desc(\"revenue\"))\n",
    "\n",
    "report = (\n",
    "    trips.filter((F.col(\"fare_amount\") > 10.0) & (F.col(\"trip_distance\") > 0))\n",
    "    .join(zones, trips[\"PULocationID\"] == zones[\"LocationID\"], \"inner\")\n",
    "    .groupBy(\"Borough\", \"Zone\")\n",
    "    .agg(\n",
    "        F.sum(\"total_amount\").alias(\"revenue\"),\n",
    "        F.count(\"*\").alias(\"trips\"),\n",
    "        F.avg(\"tip_amount\").alias(\"avg_tip\"),\n",
    "        F.avg(\"trip_distance\").alias(\"avg_distance\"),\n",
    "    )\n",
    "    .withColumn(\"rank\", F.row_number().over(window))\n",
    "    .orderBy(\"Borough\", \"rank\")\n",
    ")\n",
    "\n",
    "report.write.partitionBy(\"Borough\").mode(\"overwrite\").parquet(\n",
    "    \"s3a://warehouse/notebook_report/\"\n",
    ")\n",
    "print(f\"Wrote {report.count():,} rows to s3a://warehouse/notebook_report/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd4641cc4064e0191573fe9c69df29b",
   "metadata": {},
   "source": [
    "## Read Back Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8309879909854d7188b41380fd92a7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved = spark.read.parquet(\"s3a://warehouse/notebook_report/\")\n",
    "saved.filter(F.col(\"rank\") <= 3).orderBy(\"Borough\", \"rank\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed186c9a28b402fb0bc4494df01f08d",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1e1581032b452c9409d6c6813c49d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"SparkSession stopped.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}