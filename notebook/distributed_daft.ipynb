{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Daft on Docker Compose\n",
    "Multimodal-native DataFrame engine with a Rust core — from laptop to cluster with zero code changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Start the Ray+Daft stack and launch Jupyter:\n",
    "\n",
    "```bash\n",
    "# 1. Build images\n",
    "docker compose build\n",
    "\n",
    "# 2. Start MinIO + Ray + App\n",
    "docker compose up -d minio minio-setup ray-head app\n",
    "\n",
    "# 3. Upload sample data\n",
    "./scripts/upload-data.sh\n",
    "\n",
    "# 4. Launch Jupyter Lab\n",
    "docker compose exec app jupyter lab --ip 0.0.0.0 --port 8888 --allow-root --no-browser --notebook-dir=/app/notebook\n",
    "```\n",
    "\n",
    "Then open http://localhost:8888 in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "source": [
    "## What is Daft?\n",
    "\n",
    "Daft is a **multimodal-native DataFrame engine** with a Rust core. Key concepts:\n",
    "\n",
    "- **Lazy evaluation** — builds a query plan, executes on `.collect()` or `.show()`\n",
    "- **Rust-native ops** — `url.download()`, `image.decode()`, `image.resize()` run in Rust, not Python\n",
    "- **Streaming execution** — bounded memory via the Swordfish scheduler\n",
    "- **Seamless scaling** — same code runs locally or on a Ray cluster (`DAFT_RUNNER=ray`)\n",
    "- **Class UDFs** — GPU models loaded once per worker, reused across batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "```\n",
    "Daft Client (app) → Ray Backend (ray-head, GPU) → MinIO (S3)\n",
    "```\n",
    "\n",
    "Daft uses Ray as its distributed execution backend. The `DAFT_RUNNER=ray` env var enables this transparently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import daft\n",
    "from daft import col\n",
    "\n",
    "# S3/MinIO configuration\n",
    "io_config = daft.io.IOConfig(\n",
    "    s3=daft.io.S3Config(\n",
    "        endpoint_url=os.environ.get(\"AWS_ENDPOINT_URL\", \"http://minio:9000\"),\n",
    "        key_id=os.environ.get(\"AWS_ACCESS_KEY_ID\", \"minioadmin\"),\n",
    "        access_key=os.environ.get(\"AWS_SECRET_ACCESS_KEY\", \"minioadmin\"),\n",
    "        region_name=\"us-east-1\",\n",
    "    )\n",
    ")\n",
    "daft.set_planning_config(default_io_config=io_config)\n",
    "\n",
    "print(f\"Daft runner: {os.environ.get('DAFT_RUNNER', 'py')}\")\n",
    "print(f\"Daft version: {daft.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## Read Parquet from S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi = daft.read_parquet(\"s3://lake/taxi/*.parquet\")\n",
    "taxi.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "source": [
    "## DataFrame Operations\n",
    "\n",
    "Daft's expression API supports filter, select, groupby, and aggregation — all lazily evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter high-value trips\n",
    "high_value = taxi.where((col(\"fare_amount\") > 10.0) & (col(\"trip_distance\") > 5.0))\n",
    "high_value.select(\"trip_distance\", \"fare_amount\", \"tip_amount\", \"total_amount\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue by payment type\n",
    "(\n",
    "    taxi.groupby(\"payment_type\")\n",
    "    .agg(\n",
    "        col(\"total_amount\").sum().alias(\"total_revenue\"),\n",
    "        col(\"total_amount\").count().alias(\"trip_count\"),\n",
    "        col(\"tip_amount\").mean().alias(\"avg_tip\"),\n",
    "    )\n",
    "    .sort(col(\"total_revenue\"), desc=True)\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "source": [
    "## Read Image Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = daft.read_parquet(\"s3://bucket/image_metadata.parquet\")\n",
    "images.schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bbdb311c014d738909a11f9e486628",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43b363d81ae4b689946ece5c682cd59",
   "metadata": {},
   "source": [
    "## Multimodal Pipeline — Rust-Native Ops\n",
    "\n",
    "Download, decode, and resize images using Daft's built-in Rust operations.\n",
    "No Python Pillow needed — these ops run in parallel native Rust threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a65eabff63a45729fe45fb5ade58bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed = (\n",
    "    images.with_column(\"image_bytes\", col(\"image_url\").url.download())\n",
    "    .with_column(\"image\", col(\"image_bytes\").image.decode())\n",
    "    .with_column(\"resized\", col(\"image\").image.resize(224, 224))\n",
    ")\n",
    "print(f\"Processed {processed.count_rows()} images\")\n",
    "processed.select(\"image_url\", \"resized\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3933fab20d04ec698c2621248eb3be0",
   "metadata": {},
   "source": [
    "## GPU Embedding — CLIP via Class UDF\n",
    "\n",
    "The `ImageEmbedder` loads CLIP ViT-base-patch32 **once per worker** and encodes image batches on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4641cc4064e0191573fe9c69df29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "@daft.cls\n",
    "class ImageEmbedder:\n",
    "    \"\"\"GPU-bound: CLIP model loaded once, encode batches of images.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        import logging\n",
    "        import torch\n",
    "\n",
    "        logging.getLogger(\"transformers\").setLevel(logging.ERROR)\n",
    "        from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\n",
    "            self.device\n",
    "        )\n",
    "        self.processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.model.eval()\n",
    "\n",
    "    @daft.method.batch(\n",
    "        return_dtype=daft.DataType.fixed_size_list(daft.DataType.float32(), 512)\n",
    "    )\n",
    "    def __call__(self, image_bytes_col):\n",
    "        import io\n",
    "        import torch\n",
    "        from PIL import Image\n",
    "\n",
    "        default_embedding = np.zeros(512, dtype=np.float32)\n",
    "        embeddings = []\n",
    "        for img_bytes in image_bytes_col.to_pylist():\n",
    "            if img_bytes is None:\n",
    "                embeddings.append(default_embedding)\n",
    "                continue\n",
    "            try:\n",
    "                img = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
    "                inputs = self.processor(images=img, return_tensors=\"pt\").to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    features = self.model.get_image_features(**inputs)\n",
    "                embeddings.append(features[0].cpu().numpy().astype(np.float32))\n",
    "            except Exception:\n",
    "                embeddings.append(default_embedding)\n",
    "        return np.array(embeddings, dtype=np.float32)\n",
    "\n",
    "\n",
    "embedder = ImageEmbedder()\n",
    "\n",
    "embedded = (\n",
    "    images.with_column(\"image_bytes\", col(\"image_url\").url.download())\n",
    "    .with_column(\"embedding\", embedder(col(\"image_bytes\")))\n",
    "    .exclude(\"image_bytes\")\n",
    ")\n",
    "embedded.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8309879909854d7188b41380fd92a7c3",
   "metadata": {},
   "source": [
    "## Write and Verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed186c9a28b402fb0bc4494df01f08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded.write_parquet(\"s3://bucket/notebook_embeddings/\")\n",
    "print(\"Written to s3://bucket/notebook_embeddings/\")\n",
    "\n",
    "# Read back\n",
    "saved = daft.read_parquet(\"s3://bucket/notebook_embeddings/\")\n",
    "print(f\"Read back {saved.count_rows():,} rows\")\n",
    "saved.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e1581032b452c9409d6c6813c49d1",
   "metadata": {},
   "source": [
    "## Zero-Copy Interop — Daft to Arrow to Polars\n",
    "\n",
    "Daft DataFrames can be converted to Arrow tables with zero copy, enabling interop with any Arrow-compatible library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379cbbc1e968416e875cc15c1202d7eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daft → Arrow → Polars (zero-copy where possible)\n",
    "arrow_table = taxi.limit(1000).to_arrow()\n",
    "print(f\"Arrow table: {arrow_table.num_rows} rows, {arrow_table.num_columns} columns\")\n",
    "\n",
    "try:\n",
    "    import polars as pl\n",
    "\n",
    "    polars_df = pl.from_arrow(arrow_table)\n",
    "    print(f\"Polars DataFrame: {polars_df.shape}\")\n",
    "    print(polars_df.head(3))\n",
    "except ImportError:\n",
    "    print(\"Polars not installed — skipping interop demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277c27b1587741f2af2001be3712ef0d",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "No explicit cleanup needed — Daft uses the Ray cluster managed by Docker Compose.\n",
    "Stop with `docker compose down`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}