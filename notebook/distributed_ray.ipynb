{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fb27b941602401d91542211134fc71a",
   "metadata": {},
   "source": [
    "# Ray Data on Docker Compose\n",
    "Streaming GPU inference with Ray Data — built for GPU saturation and heterogeneous scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acae54e37e7d407bbb7b55eff062a284",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Start the Ray stack and launch Jupyter:\n",
    "\n",
    "```bash\n",
    "# 1. Build images\n",
    "docker compose build\n",
    "\n",
    "# 2. Start MinIO + Ray + App\n",
    "docker compose up -d minio minio-setup ray-head app\n",
    "\n",
    "# 3. Upload sample data\n",
    "./scripts/upload-data.sh\n",
    "\n",
    "# 4. Launch Jupyter Lab\n",
    "docker compose exec app jupyter lab --ip 0.0.0.0 --port 8888 --allow-root --no-browser --notebook-dir=/app/notebook\n",
    "```\n",
    "\n",
    "Then open http://localhost:8888 in your browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a63283cbaf04dbcab1f6479b197f3a8",
   "metadata": {},
   "source": [
    "## What is Ray Data?\n",
    "\n",
    "Ray Data is a streaming data framework designed for **GPU-heavy ML workloads**. Key concepts:\n",
    "\n",
    "- **Datasets** — distributed, streaming collections of Arrow-backed rows\n",
    "- **map_batches** — the core operation: apply a function to batches of data\n",
    "- **ActorPoolStrategy** — persistent GPU workers with model loaded once per actor\n",
    "- **Streaming execution** — bounded memory, backpressure-aware\n",
    "- **Heterogeneous scheduling** — CPU preprocessing → GPU inference seamlessly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0d8092fe74a7c96281538738b07e2",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "```\n",
    "Client (app) → Ray Head (GPU execution) → MinIO (S3 storage)\n",
    "```\n",
    "\n",
    "The app connects to the Ray cluster as a client. Ray schedules tasks on the head node (or workers). Data reads/writes go through MinIO."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72eea5119410473aa328ad9291626812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-02-06 22:45:27,406\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2026-02-06 22:45:27,421\tINFO client_builder.py:241 -- Passing the following kwargs to ray.init() on the server: log_to_driver\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster resources:\n",
      "  CPU: 8.0\n",
      "  GPU: 1.0\n",
      "  accelerator_type:G: 1.0\n",
      "  memory: 21260797952.0\n",
      "  node:172.18.0.5: 1.0\n",
      "  node:__internal_head__: 1.0\n",
      "  object_store_memory: 4000000000.0\n",
      "\n",
      "✓ Ray initialized with MinIO configuration\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import ray\n",
    "\n",
    "# Initialize Ray with runtime environment that ensures AWS env vars are set\n",
    "ray.init(\n",
    "    \"ray://ray-head:10001\",\n",
    "    runtime_env={\n",
    "        \"env_vars\": {\n",
    "            \"AWS_ENDPOINT_URL\": \"http://minio:9000\",\n",
    "            \"AWS_ACCESS_KEY_ID\": \"minioadmin\",\n",
    "            \"AWS_SECRET_ACCESS_KEY\": \"minioadmin\",\n",
    "            \"AWS_DEFAULT_REGION\": \"us-east-1\",\n",
    "            \"AWS_REGION\": \"us-east-1\",\n",
    "        }\n",
    "    },\n",
    ")\n",
    "\n",
    "resources = ray.cluster_resources()\n",
    "print(f\"Cluster resources:\")\n",
    "for k, v in sorted(resources.items()):\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "print(\"\\n✓ Ray initialized with MinIO configuration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edb47106e1a46a883d545849b8ab81b",
   "metadata": {},
   "source": [
    "## Read Tabular Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sj8v96wjr3k",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Ready to read/write data from MinIO via Ray tasks\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Ray client mode doesn't support passing PyArrow filesystem objects directly\n",
    "# Instead, we use Ray remote tasks that create the filesystem on Ray workers\n",
    "# This avoids serialization issues while still accessing MinIO\n",
    "\n",
    "print(\"✓ Ready to read/write data from MinIO via Ray tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10185d26023b46108eb7d9f57d49d2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "VendorID: int32\n",
      "tpep_pickup_datetime: timestamp[us]\n",
      "tpep_dropoff_datetime: timestamp[us]\n",
      "passenger_count: int64\n",
      "trip_distance: double\n",
      "RatecodeID: int64\n",
      "store_and_fwd_flag: large_string\n",
      "PULocationID: int32\n",
      "DOLocationID: int32\n",
      "payment_type: int64\n",
      "fare_amount: double\n",
      "extra: double\n",
      "mta_tax: double\n",
      "tip_amount: double\n",
      "tolls_amount: double\n",
      "improvement_surcharge: double\n",
      "total_amount: double\n",
      "congestion_surcharge: double\n",
      "Airport_fee: double\n",
      "\n",
      "Count: 2,964,624\n",
      "\n",
      "Sample rows:\n",
      "1. {'VendorID': 2, 'tpep_pickup_datetime': datetime.datetime(2024, 1, 1, 0, 57, 55), 'tpep_dropoff_datetime': datetime.datetime(2024, 1, 1, 1, 17, 43), 'passenger_count': 1, 'trip_distance': 1.72, 'RatecodeID': 1, 'store_and_fwd_flag': 'N', 'PULocationID': 186, 'DOLocationID': 79, 'payment_type': 2, 'fare_amount': 17.7, 'extra': 1.0, 'mta_tax': 0.5, 'tip_amount': 0.0, 'tolls_amount': 0.0, 'improvement_surcharge': 1.0, 'total_amount': 22.7, 'congestion_surcharge': 2.5, 'Airport_fee': 0.0}\n",
      "2. {'VendorID': 1, 'tpep_pickup_datetime': datetime.datetime(2024, 1, 1, 0, 3), 'tpep_dropoff_datetime': datetime.datetime(2024, 1, 1, 0, 9, 36), 'passenger_count': 1, 'trip_distance': 1.8, 'RatecodeID': 1, 'store_and_fwd_flag': 'N', 'PULocationID': 140, 'DOLocationID': 236, 'payment_type': 1, 'fare_amount': 10.0, 'extra': 3.5, 'mta_tax': 0.5, 'tip_amount': 3.75, 'tolls_amount': 0.0, 'improvement_surcharge': 1.0, 'total_amount': 18.75, 'congestion_surcharge': 2.5, 'Airport_fee': 0.0}\n",
      "3. {'VendorID': 1, 'tpep_pickup_datetime': datetime.datetime(2024, 1, 1, 0, 17, 6), 'tpep_dropoff_datetime': datetime.datetime(2024, 1, 1, 0, 35, 1), 'passenger_count': 1, 'trip_distance': 4.7, 'RatecodeID': 1, 'store_and_fwd_flag': 'N', 'PULocationID': 236, 'DOLocationID': 79, 'payment_type': 1, 'fare_amount': 23.3, 'extra': 3.5, 'mta_tax': 0.5, 'tip_amount': 3.0, 'tolls_amount': 0.0, 'improvement_surcharge': 1.0, 'total_amount': 31.3, 'congestion_surcharge': 2.5, 'Airport_fee': 0.0}\n",
      "4. {'VendorID': 1, 'tpep_pickup_datetime': datetime.datetime(2024, 1, 1, 0, 36, 38), 'tpep_dropoff_datetime': datetime.datetime(2024, 1, 1, 0, 44, 56), 'passenger_count': 1, 'trip_distance': 1.4, 'RatecodeID': 1, 'store_and_fwd_flag': 'N', 'PULocationID': 79, 'DOLocationID': 211, 'payment_type': 1, 'fare_amount': 10.0, 'extra': 3.5, 'mta_tax': 0.5, 'tip_amount': 2.0, 'tolls_amount': 0.0, 'improvement_surcharge': 1.0, 'total_amount': 17.0, 'congestion_surcharge': 2.5, 'Airport_fee': 0.0}\n",
      "5. {'VendorID': 1, 'tpep_pickup_datetime': datetime.datetime(2024, 1, 1, 0, 46, 51), 'tpep_dropoff_datetime': datetime.datetime(2024, 1, 1, 0, 52, 57), 'passenger_count': 1, 'trip_distance': 0.8, 'RatecodeID': 1, 'store_and_fwd_flag': 'N', 'PULocationID': 211, 'DOLocationID': 148, 'payment_type': 1, 'fare_amount': 7.9, 'extra': 3.5, 'mta_tax': 0.5, 'tip_amount': 3.2, 'tolls_amount': 0.0, 'improvement_surcharge': 1.0, 'total_amount': 16.1, 'congestion_surcharge': 2.5, 'Airport_fee': 0.0}\n",
      "\n",
      "✓ Taxi data loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Read and display taxi data via Ray task\n",
    "@ray.remote\n",
    "def read_and_show_taxi_data():\n",
    "    import pyarrow.parquet as pq\n",
    "    import pyarrow.fs as pafs\n",
    "\n",
    "    fs = pafs.S3FileSystem(\n",
    "        endpoint_override=\"minio:9000\",\n",
    "        access_key=\"minioadmin\",\n",
    "        secret_key=\"minioadmin\",\n",
    "        scheme=\"http\",\n",
    "        region=\"us-east-1\",\n",
    "    )\n",
    "\n",
    "    # List only parquet files (exclude CSV and other formats)\n",
    "    file_info = fs.get_file_info(pafs.FileSelector(\"lake/taxi/\", recursive=False))\n",
    "    parquet_files = [\n",
    "        f.path for f in file_info if f.is_file and f.path.endswith(\".parquet\")\n",
    "    ]\n",
    "\n",
    "    if not parquet_files:\n",
    "        raise ValueError(\"No parquet files found in lake/taxi/\")\n",
    "\n",
    "    # Read all parquet files\n",
    "    table = pq.read_table(parquet_files, filesystem=fs)\n",
    "\n",
    "    # Return summary info\n",
    "    return {\n",
    "        \"schema\": str(table.schema),\n",
    "        \"count\": len(table),\n",
    "        \"sample\": table.slice(0, 5).to_pylist(),\n",
    "    }\n",
    "\n",
    "\n",
    "# Execute and display\n",
    "result_ref = read_and_show_taxi_data.remote()\n",
    "result = ray.get(result_ref)\n",
    "\n",
    "print(f\"Schema:\\n{result['schema']}\\n\")\n",
    "print(f\"Count: {result['count']:,}\\n\")\n",
    "print(\"Sample rows:\")\n",
    "for i, row in enumerate(result[\"sample\"], 1):\n",
    "    print(f\"{i}. {row}\")\n",
    "\n",
    "# Store table reference for transformations\n",
    "# For map_batches demo, we'll create dataset on-cluster\n",
    "print(\"\\n✓ Taxi data loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763a12b2bbd4a93a75aff182afb95dc",
   "metadata": {},
   "source": [
    "## Basic Transformations\n",
    "\n",
    "`map_batches` applies a function to each batch. For CPU transforms, no special config needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7623eae2785240b9bd12b16a66d81610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data with tip percentage:\n",
      "1. Fare: $17.70, Tip: $0.00, Tip %: 0.0%\n",
      "2. Fare: $10.00, Tip: $3.75, Tip %: 37.5%\n",
      "3. Fare: $23.30, Tip: $3.00, Tip %: 12.9%\n",
      "4. Fare: $10.00, Tip: $2.00, Tip %: 20.0%\n",
      "5. Fare: $7.90, Tip: $3.20, Tip %: 40.5%\n",
      "6. Fare: $29.60, Tip: $6.90, Tip %: 23.3%\n",
      "7. Fare: $45.70, Tip: $10.00, Tip %: 21.9%\n",
      "8. Fare: $25.40, Tip: $0.00, Tip %: 0.0%\n",
      "9. Fare: $31.00, Tip: $0.00, Tip %: 0.0%\n",
      "10. Fare: $3.00, Tip: $0.00, Tip %: 0.0%\n"
     ]
    }
   ],
   "source": [
    "# Transformations demo - run on Ray cluster\n",
    "@ray.remote\n",
    "def transform_taxi_data():\n",
    "    import pyarrow.parquet as pq\n",
    "    import pyarrow.fs as pafs\n",
    "    import pyarrow.compute as pc\n",
    "\n",
    "    fs = pafs.S3FileSystem(\n",
    "        endpoint_override=\"minio:9000\",\n",
    "        access_key=\"minioadmin\",\n",
    "        secret_key=\"minioadmin\",\n",
    "        scheme=\"http\",\n",
    "        region=\"us-east-1\",\n",
    "    )\n",
    "\n",
    "    # Read parquet files\n",
    "    file_info = fs.get_file_info(pafs.FileSelector(\"lake/taxi/\", recursive=False))\n",
    "    parquet_files = [\n",
    "        f.path for f in file_info if f.is_file and f.path.endswith(\".parquet\")\n",
    "    ]\n",
    "    table = pq.read_table(parquet_files, filesystem=fs)\n",
    "\n",
    "    # Add tip percentage column using PyArrow compute\n",
    "    fare = table.column(\"fare_amount\")\n",
    "    tip = table.column(\"tip_amount\")\n",
    "\n",
    "    # Compute tip_pct: (tip / fare * 100) where fare > 0, else 0\n",
    "    tip_pct = pc.if_else(\n",
    "        pc.greater(fare, 0), pc.multiply(pc.divide(tip, fare), 100), 0.0\n",
    "    )\n",
    "\n",
    "    # Add column to table\n",
    "    table = table.append_column(\"tip_pct\", tip_pct)\n",
    "\n",
    "    # Return sample with selected columns\n",
    "    sample = table.select([\"fare_amount\", \"tip_amount\", \"tip_pct\"]).slice(0, 10)\n",
    "    return sample.to_pylist()\n",
    "\n",
    "\n",
    "result_ref = transform_taxi_data.remote()\n",
    "transformed_data = ray.get(result_ref)\n",
    "\n",
    "print(\"Transformed data with tip percentage:\")\n",
    "for i, row in enumerate(transformed_data, 1):\n",
    "    print(\n",
    "        f\"{i}. Fare: ${row['fare_amount']:.2f}, Tip: ${row['tip_amount']:.2f}, Tip %: {row['tip_pct']:.1f}%\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdc8c89c7104fffa095e18ddfef8986",
   "metadata": {},
   "source": [
    "## Read Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b118ea5561624da68c537baed56e602f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image count: 100\n",
      "\n",
      "Sample images:\n",
      "1. Path: bucket/images/food_00000.jpg, Shape: (512, 384, 3)\n",
      "2. Path: bucket/images/food_00001.jpg, Shape: (512, 512, 3)\n",
      "\n",
      "✓ Loaded 100 images for GPU inference\n"
     ]
    }
   ],
   "source": [
    "# Read images via Ray task\n",
    "@ray.remote\n",
    "def list_and_read_images():\n",
    "    import pyarrow.fs as pafs\n",
    "    import numpy as np\n",
    "    from PIL import Image\n",
    "    import io\n",
    "\n",
    "    fs = pafs.S3FileSystem(\n",
    "        endpoint_override=\"minio:9000\",\n",
    "        access_key=\"minioadmin\",\n",
    "        secret_key=\"minioadmin\",\n",
    "        scheme=\"http\",\n",
    "        region=\"us-east-1\",\n",
    "    )\n",
    "\n",
    "    # List all image files\n",
    "    file_info = fs.get_file_info(pafs.FileSelector(\"bucket/images/\", recursive=True))\n",
    "    image_files = [\n",
    "        f.path\n",
    "        for f in file_info\n",
    "        if f.is_file and f.path.endswith((\".jpg\", \".jpeg\", \".png\"))\n",
    "    ]\n",
    "\n",
    "    # Read images (limit for demo)\n",
    "    images_data = []\n",
    "    for path in image_files[:100]:\n",
    "        try:\n",
    "            with fs.open_input_file(path) as f:\n",
    "                img_bytes = f.read()\n",
    "                img = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
    "                img_array = np.array(img)\n",
    "                images_data.append(\n",
    "                    {\"image\": img_array, \"path\": path, \"shape\": img_array.shape}\n",
    "                )\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {path}: {e}\")\n",
    "\n",
    "    return images_data\n",
    "\n",
    "\n",
    "# Get images\n",
    "images_data_ref = list_and_read_images.remote()\n",
    "images_data = ray.get(images_data_ref)\n",
    "\n",
    "print(f\"Image count: {len(images_data)}\")\n",
    "print(\"\\nSample images:\")\n",
    "for i, img_info in enumerate(images_data[:2], 1):\n",
    "    print(f\"{i}. Path: {img_info['path']}, Shape: {img_info['shape']}\")\n",
    "\n",
    "print(f\"\\n✓ Loaded {len(images_data)} images for GPU inference\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938c804e27f84196a10c8828c723f798",
   "metadata": {},
   "source": [
    "## GPU Inference with ActorPoolStrategy\n",
    "\n",
    "The `ImageClassifier` loads ResNet-50 **once per actor** and reuses it across batches.\n",
    "This avoids the cost of loading a model for every batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "504fb2a444614c0babb325280ed9130a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting GPU inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/app/.venv/lib/python3.11/site-packages/ray/util/client/worker.py:619: UserWarning: More than 10MB of messages have been created to schedule tasks on the server. This can be slow on Ray Client due to communication overhead over the network. If you're running many fine-grained tasks, consider running them inside a single remote function. See the section on \"Too fine-grained tasks\" in the Ray Design Patterns document for more details: https://docs.ray.io/en/latest/ray-core/patterns/too-fine-grained-tasks.html. If your functions frequently use large objects, consider storing the objects remotely with ray.put. An example of this is shown in the \"Closure capture of large / unserializable object\" section of the Ray Design Patterns document, available here: https://docs.ray.io/en/latest/ray-core/patterns/closure-capture-large-objects.html\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(classify_images_on_gpu pid=3079)\u001b[0m [GPU Inference] Loading ResNet-50 on cuda\n",
      "\u001b[36m(classify_images_on_gpu pid=3079)\u001b[0m Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.00/97.8M [00:00<?, ?B/s]\n",
      "  2%|▏         | 2.12M/97.8M [00:00<00:04, 22.1MB/s]\n",
      "  7%|▋         | 6.38M/97.8M [00:00<00:02, 35.3MB/s]\n",
      " 13%|█▎        | 13.1M/97.8M [00:00<00:01, 51.3MB/s]\n",
      " 19%|█▊        | 18.2M/97.8M [00:00<00:01, 52.1MB/s]\n",
      " 24%|██▍       | 23.4M/97.8M [00:00<00:01, 52.6MB/s]\n",
      " 29%|██▉       | 28.6M/97.8M [00:00<00:01, 53.0MB/s]\n",
      " 35%|███▍      | 33.8M/97.8M [00:00<00:01, 53.1MB/s]\n",
      " 40%|███▉      | 38.9M/97.8M [00:00<00:01, 53.2MB/s]\n",
      " 45%|████▍     | 44.0M/97.8M [00:00<00:01, 53.4MB/s]\n",
      " 50%|█████     | 49.1M/97.8M [00:01<00:00, 53.5MB/s]\n",
      " 55%|█████▌    | 54.2M/97.8M [00:01<00:00, 53.4MB/s]\n",
      " 61%|██████    | 59.9M/97.8M [00:01<00:00, 55.0MB/s]\n",
      " 67%|██████▋   | 65.1M/97.8M [00:01<00:00, 54.0MB/s]\n",
      " 73%|███████▎  | 71.2M/97.8M [00:01<00:00, 56.8MB/s]\n",
      " 78%|███████▊  | 76.8M/97.8M [00:01<00:00, 56.0MB/s]\n",
      " 84%|████████▍ | 82.1M/97.8M [00:01<00:00, 55.4MB/s]\n",
      " 89%|████████▉ | 87.5M/97.8M [00:01<00:00, 55.2MB/s]\n",
      " 95%|█████████▍| 92.9M/97.8M [00:01<00:00, 54.9MB/s]\n",
      "100%|██████████| 97.8M/97.8M [00:01<00:00, 53.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(classify_images_on_gpu pid=3079)\u001b[0m [GPU Inference] Processing 100 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(classify_images_on_gpu pid=3079)\u001b[0m /tmp/ipykernel_2148/1603373604.py:27: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:213.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Classified 100 images\n"
     ]
    }
   ],
   "source": [
    "# GPU Inference with Ray remote task\n",
    "@ray.remote(num_gpus=1)\n",
    "def classify_images_on_gpu(images_data):\n",
    "    \"\"\"Run image classification on GPU using ResNet-50.\"\"\"\n",
    "    import torch\n",
    "    from torchvision.models import ResNet50_Weights, resnet50\n",
    "    import numpy as np\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"[GPU Inference] Loading ResNet-50 on {device}\")\n",
    "\n",
    "    weights = ResNet50_Weights.DEFAULT\n",
    "    model = resnet50(weights=weights).to(device).eval()\n",
    "    preprocess = weights.transforms()\n",
    "    categories = weights.meta[\"categories\"]\n",
    "\n",
    "    print(f\"[GPU Inference] Processing {len(images_data)} images...\")\n",
    "\n",
    "    predictions = []\n",
    "    batch_size = 32\n",
    "\n",
    "    for i in range(0, len(images_data), batch_size):\n",
    "        batch = images_data[i : i + batch_size]\n",
    "\n",
    "        # Preprocess batch\n",
    "        tensors = torch.stack(\n",
    "            [\n",
    "                preprocess(torch.from_numpy(img_data[\"image\"]).permute(2, 0, 1))\n",
    "                for img_data in batch\n",
    "            ]\n",
    "        ).to(device)\n",
    "\n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            logits = model(tensors)\n",
    "\n",
    "        # Get predictions\n",
    "        top_idx = logits.argmax(dim=1).cpu().numpy()\n",
    "        confidences = logits.softmax(dim=1).max(dim=1).values.cpu().numpy()\n",
    "\n",
    "        for j, img_data in enumerate(batch):\n",
    "            predictions.append(\n",
    "                {\n",
    "                    \"path\": img_data[\"path\"],\n",
    "                    \"prediction\": categories[top_idx[j]],\n",
    "                    \"confidence\": float(confidences[j]),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Run GPU inference\n",
    "print(\"Starting GPU inference...\")\n",
    "predictions_ref = classify_images_on_gpu.remote(images_data)\n",
    "predictions = ray.get(predictions_ref)\n",
    "\n",
    "print(f\"\\n✓ Classified {len(predictions)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bbdb311c014d738909a11f9e486628",
   "metadata": {},
   "source": [
    "## Inspect Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b43b363d81ae4b689946ece5c682cd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions:\n",
      "1. spatula                        (confidence: 0.2423)\n",
      "2. French loaf                    (confidence: 0.3737)\n",
      "3. eggnog                         (confidence: 0.0718)\n",
      "4. plate                          (confidence: 0.5072)\n",
      "5. French loaf                    (confidence: 0.1651)\n",
      "6. tray                           (confidence: 0.1228)\n",
      "7. chocolate sauce                (confidence: 0.1451)\n",
      "8. pretzel                        (confidence: 0.1955)\n",
      "9. pretzel                        (confidence: 0.1525)\n",
      "10. French loaf                    (confidence: 0.1984)\n",
      "\n",
      "Top-10 predicted classes:\n",
      "  French loaf                   : 34\n",
      "  dough                         : 15\n",
      "  chocolate sauce               : 9\n",
      "  eggnog                        : 8\n",
      "  plate                         : 7\n",
      "  pretzel                       : 4\n",
      "  tray                          : 2\n",
      "  burrito                       : 2\n",
      "  meat loaf                     : 2\n",
      "  espresso                      : 2\n",
      "\n",
      "Confidence — avg: 0.2172, min: 0.0544, max: 0.7844\n"
     ]
    }
   ],
   "source": [
    "# Display predictions\n",
    "print(\"Sample predictions:\")\n",
    "for i, pred in enumerate(predictions[:10], 1):\n",
    "    print(f\"{i}. {pred['prediction']:30s} (confidence: {pred['confidence']:.4f})\")\n",
    "\n",
    "# Class distribution\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "pred_classes = [p[\"prediction\"] for p in predictions]\n",
    "confidences = [p[\"confidence\"] for p in predictions]\n",
    "\n",
    "print(\"\\nTop-10 predicted classes:\")\n",
    "for cls, count in Counter(pred_classes).most_common(10):\n",
    "    print(f\"  {cls:30s}: {count}\")\n",
    "\n",
    "print(\n",
    "    f\"\\nConfidence — avg: {np.mean(confidences):.4f}, \"\n",
    "    f\"min: {np.min(confidences):.4f}, \"\n",
    "    f\"max: {np.max(confidences):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a65eabff63a45729fe45fb5ade58bdc",
   "metadata": {},
   "source": [
    "## Write Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3933fab20d04ec698c2621248eb3be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written 100 predictions to s3://bucket/notebook_predictions/\n",
      "Read back 100 rows\n",
      "\n",
      "Sample rows:\n",
      "1. {'path': 'bucket/images/food_00000.jpg', 'prediction': 'spatula', 'confidence': 0.24226126074790955}\n",
      "2. {'path': 'bucket/images/food_00001.jpg', 'prediction': 'French loaf', 'confidence': 0.37366271018981934}\n",
      "3. {'path': 'bucket/images/food_00002.jpg', 'prediction': 'eggnog', 'confidence': 0.0718255490064621}\n",
      "4. {'path': 'bucket/images/food_00003.jpg', 'prediction': 'plate', 'confidence': 0.5072240233421326}\n",
      "5. {'path': 'bucket/images/food_00004.jpg', 'prediction': 'French loaf', 'confidence': 0.16506265103816986}\n"
     ]
    }
   ],
   "source": [
    "# Write predictions to S3\n",
    "@ray.remote\n",
    "def write_predictions_to_s3(predictions):\n",
    "    import pyarrow as pa\n",
    "    import pyarrow.parquet as pq\n",
    "    import pyarrow.fs as pafs\n",
    "\n",
    "    fs = pafs.S3FileSystem(\n",
    "        endpoint_override=\"minio:9000\",\n",
    "        access_key=\"minioadmin\",\n",
    "        secret_key=\"minioadmin\",\n",
    "        scheme=\"http\",\n",
    "        region=\"us-east-1\",\n",
    "    )\n",
    "\n",
    "    # Convert to Arrow table and write\n",
    "    table = pa.Table.from_pylist(predictions)\n",
    "    pq.write_to_dataset(table, root_path=\"bucket/notebook_predictions/\", filesystem=fs)\n",
    "    return len(predictions)\n",
    "\n",
    "\n",
    "count_ref = write_predictions_to_s3.remote(predictions)\n",
    "count = ray.get(count_ref)\n",
    "print(f\"Written {count:,} predictions to s3://bucket/notebook_predictions/\")\n",
    "\n",
    "\n",
    "# Read back to verify\n",
    "@ray.remote\n",
    "def read_and_show_predictions():\n",
    "    import pyarrow.parquet as pq\n",
    "    import pyarrow.fs as pafs\n",
    "\n",
    "    fs = pafs.S3FileSystem(\n",
    "        endpoint_override=\"minio:9000\",\n",
    "        access_key=\"minioadmin\",\n",
    "        secret_key=\"minioadmin\",\n",
    "        scheme=\"http\",\n",
    "        region=\"us-east-1\",\n",
    "    )\n",
    "\n",
    "    table = pq.read_table(\"bucket/notebook_predictions/\", filesystem=fs)\n",
    "    return {\"count\": len(table), \"sample\": table.slice(0, 5).to_pylist()}\n",
    "\n",
    "\n",
    "result_ref = read_and_show_predictions.remote()\n",
    "result = ray.get(result_ref)\n",
    "\n",
    "print(f\"Read back {result['count']:,} rows\")\n",
    "print(\"\\nSample rows:\")\n",
    "for i, row in enumerate(result[\"sample\"], 1):\n",
    "    print(f\"{i}. {row}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd4641cc4064e0191573fe9c69df29b",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8309879909854d7188b41380fd92a7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray disconnected.\n"
     ]
    }
   ],
   "source": [
    "ray.shutdown()\n",
    "print(\"Ray disconnected.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
